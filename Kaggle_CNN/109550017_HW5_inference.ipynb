{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install opencv-python\n","!pip install numpy\n","!pip install torch"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.524461Z","iopub.status.busy":"2022-12-18T07:36:42.524101Z","iopub.status.idle":"2022-12-18T07:36:42.530275Z","shell.execute_reply":"2022-12-18T07:36:42.529095Z","shell.execute_reply.started":"2022-12-18T07:36:42.524430Z"},"trusted":true},"outputs":[],"source":["import csv\n","import cv2\n","import numpy as np\n","import random\n","import os\n","\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.554279Z","iopub.status.busy":"2022-12-18T07:36:42.554021Z","iopub.status.idle":"2022-12-18T07:36:42.559912Z","shell.execute_reply":"2022-12-18T07:36:42.558776Z","shell.execute_reply.started":"2022-12-18T07:36:42.554256Z"},"trusted":true},"outputs":[],"source":["TRAIN_PATH = \"/kaggle/input/captcha-hacker/train\"\n","TEST_PATH = \"/kaggle/input/captcha-hacker/test\"\n","# device = \"cpu\"\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# try device = \"cuda\" \n","# and change your settings/accelerator to GPU if you want it to run faster"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.562467Z","iopub.status.busy":"2022-12-18T07:36:42.561527Z","iopub.status.idle":"2022-12-18T07:36:42.716938Z","shell.execute_reply":"2022-12-18T07:36:42.716040Z","shell.execute_reply.started":"2022-12-18T07:36:42.562434Z"},"trusted":true},"outputs":[],"source":["train_data = []\n","val_data = []\n","test_data = []\n","\n","# with open(f'{TRAIN_PATH}/annotations.csv', newline='') as csvfile:\n","#     for row in csv.reader(csvfile, delimiter=','):\n","#         if random.random() < 0.7:\n","#             train_data.append(row)\n","#         else:\n","#             val_data.append(row)\n","\n","# with open(f'{TEST_PATH}/../sample_submission.csv', newline='') as csvfile:\n","#     for row in csv.reader(csvfile, delimiter=','):\n","#         test_data.append(row)\n","\n","with open(f'{TRAIN_PATH}/annotations.csv', newline='') as csvfile:\n","    for row in csv.reader(csvfile, delimiter=','):\n","        train_data.append(row)\n","\n","with open(f'{TEST_PATH}/../sample_submission.csv', newline='') as csvfile:\n","    for row in csv.reader(csvfile, delimiter=','):\n","        test_data.append(row)"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.721066Z","iopub.status.busy":"2022-12-18T07:36:42.720761Z","iopub.status.idle":"2022-12-18T07:36:42.727052Z","shell.execute_reply":"2022-12-18T07:36:42.726027Z","shell.execute_reply.started":"2022-12-18T07:36:42.721040Z"},"trusted":true},"outputs":[],"source":["def load_checkpoint(filepath):\n","    checkpoint = torch.load(filepath)\n","    model = checkpoint['model']\n","    model.load_state_dict(checkpoint['state_dict'])\n","    for parameter in model.parameters():\n","        parameter.requires_grad = False\n","    \n","    model.eval()\n","    \n","    return model"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.729192Z","iopub.status.busy":"2022-12-18T07:36:42.728307Z","iopub.status.idle":"2022-12-18T07:36:42.914868Z","shell.execute_reply":"2022-12-18T07:36:42.913721Z","shell.execute_reply.started":"2022-12-18T07:36:42.729156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model1(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout1): Dropout(p=0.2, inplace=False)\n","  (conv3): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv4): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout2): Dropout(p=0.3, inplace=False)\n","  (conv5): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv6): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout3): Dropout(p=0.4, inplace=False)\n","  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n","  (relu3): ReLU()\n","  (dropout4): Dropout(p=0.5, inplace=False)\n","  (fc2): Linear(in_features=512, out_features=10, bias=True)\n","  (output): Softmax(dim=1)\n",")\n","Model2(\n","  (conv1): Sequential(\n","    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout1): Dropout(p=0.2, inplace=False)\n","  (conv3): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv4): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout2): Dropout(p=0.3, inplace=False)\n","  (conv5): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv6): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout3): Dropout(p=0.4, inplace=False)\n","  (fc11): Linear(in_features=2048, out_features=512, bias=True)\n","  (relu31): ReLU()\n","  (dropout41): Dropout(p=0.5, inplace=False)\n","  (fc21): Linear(in_features=512, out_features=72, bias=True)\n","  (fc12): Linear(in_features=2048, out_features=512, bias=True)\n","  (relu32): ReLU()\n","  (dropout42): Dropout(p=0.5, inplace=False)\n","  (fc22): Linear(in_features=512, out_features=72, bias=True)\n","  (output): Softmax(dim=1)\n",")\n","Model3(\n","  (conv1): Sequential(\n","    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout1): Dropout(p=0.2, inplace=False)\n","  (conv3): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv4): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout2): Dropout(p=0.3, inplace=False)\n","  (conv5): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (conv6): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","  )\n","  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (dropout3): Dropout(p=0.4, inplace=False)\n","  (fc11): Linear(in_features=2048, out_features=512, bias=True)\n","  (relu31): ReLU()\n","  (dropout41): Dropout(p=0.5, inplace=False)\n","  (fc21): Linear(in_features=512, out_features=144, bias=True)\n","  (fc12): Linear(in_features=2048, out_features=512, bias=True)\n","  (relu32): ReLU()\n","  (dropout42): Dropout(p=0.5, inplace=False)\n","  (fc22): Linear(in_features=512, out_features=144, bias=True)\n","  (fc13): Linear(in_features=2048, out_features=512, bias=True)\n","  (relu33): ReLU()\n","  (dropout43): Dropout(p=0.5, inplace=False)\n","  (fc23): Linear(in_features=512, out_features=144, bias=True)\n","  (fc14): Linear(in_features=2048, out_features=512, bias=True)\n","  (relu34): ReLU()\n","  (dropout44): Dropout(p=0.5, inplace=False)\n","  (fc24): Linear(in_features=512, out_features=144, bias=True)\n","  (output): Softmax(dim=1)\n",")\n"]}],"source":["model1 = load_checkpoint('task1.pth')\n","model1 = model1.to(device)\n","print(model1)\n","model2 = load_checkpoint('task2.pth')\n","model2 = model2.to(device)\n","print(model2)\n","model3 = load_checkpoint('task3.pth')\n","model3 = model3.to(device)\n","print(model3)"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.916776Z","iopub.status.busy":"2022-12-18T07:36:42.916324Z","iopub.status.idle":"2022-12-18T07:36:42.923805Z","shell.execute_reply":"2022-12-18T07:36:42.922650Z","shell.execute_reply.started":"2022-12-18T07:36:42.916738Z"},"trusted":true},"outputs":[],"source":["LETTERSTR = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n","\n","def encode(text):\n","    labellist = []\n","    for letter in text:\n","        num = LETTERSTR.find(letter)\n","        labellist.append(num)\n","    return labellist\n","\n","LETTERSTR_char = \"0,1,2,3,4,5,6,7,8,9,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z\"\n","LETTERSTR_char = LETTERSTR_char.split(\",\")\n","LETTERSTR_index = [i for i in range(36)]\n","decode = dict(zip(LETTERSTR_index, LETTERSTR_char))"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.925987Z","iopub.status.busy":"2022-12-18T07:36:42.925597Z","iopub.status.idle":"2022-12-18T07:36:42.941981Z","shell.execute_reply":"2022-12-18T07:36:42.940956Z","shell.execute_reply.started":"2022-12-18T07:36:42.925949Z"},"trusted":true},"outputs":[],"source":["def ExtractData(data, root, task_round):\n","    filenames = np.array([sample for sample in data if sample[0].startswith(task_round)])\n","    imgs_names = filenames[:, 0]\n","    label = filenames[:, 1]\n","    imgs = np.zeros(shape=(len(filenames), 32, 32, 1))\n","    labels = np.zeros(shape=(len(filenames), len(label[0])))\n","    for i in range(len(filenames)):\n","        tmp = cv2.imread(f\"{root}/{imgs_names[i]}\")\n","        tmp = cv2.resize(tmp, (32, 32))\n","        tmp = cv2.cvtColor(tmp, cv2.COLOR_BGR2GRAY)\n","        imgs[i] = np.expand_dims(tmp, axis=2)\n","        tmp_label = encode(label[i])\n","        labels[i] = np.array(tmp_label)\n","    return imgs_names, imgs, labels\n","\n","class Task1Dataset(Dataset):\n","    def __init__(self, data, root, return_filename=False):\n","        self.data = [sample for sample in data if sample[0].startswith(\"task1\")]\n","        self.return_filename = return_filename\n","        self.root = root\n","    \n","    def __getitem__(self, index):\n","        filename, label = self.data[index]\n","        img = cv2.imread(f\"{self.root}/{filename}\")\n","        img = cv2.resize(img, (32, 32))\n","        \n","        img = torch.tensor(img).permute(2,0,1)\n","        if self.return_filename:\n","            return torch.FloatTensor((img - 128) / 128), filename\n","        else:\n","            return torch.FloatTensor((img - 128) / 128), int(label)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","class TaskDataset(Dataset):\n","    def __init__(self, x_data, y_data, filenames, return_filename=False):\n","        x_data = x_data.astype('float32')\n","        x_data = x_data / 255.0\n","        self.x_data = torch.from_numpy(x_data).permute(0, 3, 1, 2) # (N, 1, 32, 32)\n","        self.y_data = torch.from_numpy(y_data) # (N, 1)\n","        self.filenames = filenames\n","        self.return_filename = return_filename\n","        \n","    def __getitem__(self, index):\n","        if self.return_filename:\n","            return self.x_data[index], self.filenames[index] # aug_num = 5\n","        else:\n","            return self.x_data[index], self.y_data[index]\n","        \n","    def __len__(self):\n","        return len(self.x_data)"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.945538Z","iopub.status.busy":"2022-12-18T07:36:42.945147Z","iopub.status.idle":"2022-12-18T07:36:42.956905Z","shell.execute_reply":"2022-12-18T07:36:42.955868Z","shell.execute_reply.started":"2022-12-18T07:36:42.945495Z"},"trusted":true},"outputs":[],"source":["# task1 data processing\n","test_ds = Task1Dataset(test_data, root=TEST_PATH, return_filename=True)\n","test_dl = DataLoader(test_ds, batch_size=195, num_workers=2, drop_last=False, shuffle=False)"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:42.959019Z","iopub.status.busy":"2022-12-18T07:36:42.958082Z","iopub.status.idle":"2022-12-18T07:36:46.952086Z","shell.execute_reply":"2022-12-18T07:36:46.949590Z","shell.execute_reply.started":"2022-12-18T07:36:42.958984Z"},"trusted":true},"outputs":[],"source":["# predict test1 write to csv\n","csv_writer = csv.writer(open('submission.csv', 'w', newline=''))\n","csv_writer.writerow([\"filename\", \"label\"])\n","\n","# model.eval()\n","for image, filenames in test_dl:\n","    image = image.to(device)\n","    \n","    pred = model1(image)\n","    pred = torch.argmax(pred, dim=1)\n","    \n","    for i in range(len(filenames)):\n","        csv_writer.writerow([filenames[i], str(pred[i].item())])"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:46.956075Z","iopub.status.busy":"2022-12-18T07:36:46.955252Z","iopub.status.idle":"2022-12-18T07:36:49.343482Z","shell.execute_reply":"2022-12-18T07:36:49.342280Z","shell.execute_reply.started":"2022-12-18T07:36:46.956031Z"},"trusted":true},"outputs":[],"source":["# task2 data processing\n","filename_test, x_test, y_test = ExtractData(test_data, root=TEST_PATH, task_round=\"task2\")\n","test_ds = TaskDataset(x_test, y_test, filename_test, return_filename=True)\n","test_dl = DataLoader(test_ds, batch_size=75, num_workers=2, drop_last=False, shuffle=False)"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:49.346467Z","iopub.status.busy":"2022-12-18T07:36:49.344767Z","iopub.status.idle":"2022-12-18T07:36:49.713290Z","shell.execute_reply":"2022-12-18T07:36:49.711940Z","shell.execute_reply.started":"2022-12-18T07:36:49.346348Z"},"trusted":true},"outputs":[],"source":["# predict test2 write to csv\n","if os.path.exists('submission.csv'):\n","    csv_writer = csv.writer(open('submission.csv', 'a', newline=''))\n","else:\n","    csv_writer = csv.writer(open('submission.csv', 'w', newline=''))\n","    csv_writer.writerow([\"filename\", \"label\"])\n","\n","\n","# model.eval()\n","for image, filenames in test_dl:\n","    image = image.to(device)\n","    \n","    pred = model2(image)\n","    pred1 = torch.argmax(pred[0], dim=1)\n","    pred2 = torch.argmax(pred[1], dim=1)\n","    for i in range(len(filenames)):\n","        cur_string = decode[pred1[i].item()]+decode[pred2[i].item()]\n","        csv_writer.writerow([filenames[i], cur_string])"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:49.717090Z","iopub.status.busy":"2022-12-18T07:36:49.716274Z","iopub.status.idle":"2022-12-18T07:36:50.746765Z","shell.execute_reply":"2022-12-18T07:36:50.745708Z","shell.execute_reply.started":"2022-12-18T07:36:49.717045Z"},"trusted":true},"outputs":[],"source":["# task3 data processing\n","filename_test, x_test, y_test = ExtractData(test_data, root=TEST_PATH, task_round=\"task3\")\n","test_ds = TaskDataset(x_test, y_test, filename_test, return_filename=True)\n","test_dl = DataLoader(test_ds, batch_size=30, num_workers=2, drop_last=False, shuffle=False)"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2022-12-18T07:36:50.748941Z","iopub.status.busy":"2022-12-18T07:36:50.748519Z","iopub.status.idle":"2022-12-18T07:36:51.037228Z","shell.execute_reply":"2022-12-18T07:36:51.036001Z","shell.execute_reply.started":"2022-12-18T07:36:50.748907Z"},"trusted":true},"outputs":[],"source":["# predict task3 write to csv\n","csv_writer = csv.writer(open('submission.csv', 'a', newline=''))\n","\n","# model.eval()\n","for image, filenames in test_dl:\n","    image = image.to(device)\n","    \n","    pred = model3(image)\n","    pred1 = torch.argmax(pred[0], dim=1)\n","    pred2 = torch.argmax(pred[1], dim=1)\n","    pred3 = torch.argmax(pred[2], dim=1)\n","    pred4 = torch.argmax(pred[3], dim=1)\n","    \n","    for i in range(len(filenames)):\n","        cur_string = decode[pred1[i].item()]+decode[pred2[i].item()]+decode[pred3[i].item()]+decode[pred4[i].item()]\n","        csv_writer.writerow([filenames[i], cur_string])\n","        \n","    \"\"\"\n","    for filename, value in filenames:\n","        cur_string = decode[pred1[i].item()]+decode[pred2[i].item()]+decode[pred3[i].item()]+decode[pred4[i].item()]\n","        csv_writer.writerow(filename, value)\n","    \"\"\""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
