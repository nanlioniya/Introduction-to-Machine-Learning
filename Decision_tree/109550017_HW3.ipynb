{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding sample weights to evaluation causes the formulation probability \n",
    "# to differ, for sample weights != none, \n",
    "# probability is (sample_weight of same label)/(sum of sample weights)\n",
    "\n",
    "\n",
    "# gini = 1 - sum(pi**2, pi in feature i)\n",
    "def gini(sequence, sample_weight=None):\n",
    "    \n",
    "    values, cnt = np.unique(sequence, return_counts=True)\n",
    "    gini_val = 0\n",
    "    if sample_weight is None:\n",
    "        p = cnt/sequence.shape[0]\n",
    "        gini_val = np.sum([pi**2 for pi in p])\n",
    "    \n",
    "    else:\n",
    "        weight_sum = sum(sample_weight)\n",
    "        for value in values:\n",
    "            idx = [idx for idx, x in enumerate(sequence) if x == value]\n",
    "            count = np.sum(sample_weight[idx])\n",
    "            gini_val += (count / weight_sum)**2\n",
    "    \n",
    "    return 1 - gini_val\n",
    "\n",
    "\n",
    "# entropy = -sum(pj*log(pj))\n",
    "def entropy(sequence, sample_weight=None):\n",
    "    \n",
    "    values, cnt = np.unique(sequence, return_counts=True)\n",
    "    entropy_val = 0\n",
    "    if sample_weight is None:\n",
    "        p = cnt/sequence.shape[0]\n",
    "        entropy_val = -1*np.sum([pj*np.log2(pj) for pj in p]) \n",
    "    \n",
    "    else:\n",
    "        weight_sum = sum(sample_weight)\n",
    "        \n",
    "        for value in values:\n",
    "            idx = [idx for idx, x in enumerate(sequence) if x == value]\n",
    "            count = np.sum(sample_weight[idx])\n",
    "            entropy_val += -(count / weight_sum) * np.log2(count / weight_sum)\n",
    "    \n",
    "    return entropy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1583</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>148</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>942</td>\n",
       "      <td>1651</td>\n",
       "      <td>1704</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.8</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>1538</td>\n",
       "      <td>2459</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>1504</td>\n",
       "      <td>1799</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>164</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>1394</td>\n",
       "      <td>1944</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>695</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.6</td>\n",
       "      <td>196</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1649</td>\n",
       "      <td>1829</td>\n",
       "      <td>2855</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1583     1          2.1         1  11       0          14    0.7   \n",
       "1            745     1          0.6         1   5       0          35    0.8   \n",
       "2            832     0          0.7         1   2       1          39    0.7   \n",
       "3           1175     1          1.3         0   2       0          19    0.3   \n",
       "4            695     0          0.5         0  18       1          12    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        148        7  ...        942      1651  1704    17    13          2   \n",
       "1        102        8  ...         89      1538  2459    14     1         16   \n",
       "2        103        4  ...        125      1504  1799     5     2         11   \n",
       "3        164        7  ...        873      1394  1944     9     4          9   \n",
       "4        196        2  ...       1649      1829  2855    16    13          7   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        1             0     1            1  \n",
       "1        1             1     0            0  \n",
       "2        1             0     1            0  \n",
       "3        1             1     0            0  \n",
       "4        1             1     1            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "train_df.head() # 21 columns = 20 for x_data, 1 for y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.iloc[:, 0:-1].values\n",
    "y_train = train_df.iloc[:, -1].values\n",
    "x_test = val_df.iloc[:, 0:-1].values\n",
    "y_test = val_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe nodes in the decision tree\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*, value=None):\n",
    "        \n",
    "        # for decision node\n",
    "        self.feature = feature # the feature this node is divided with\n",
    "        self.threshold = threshold # the threshold this node is divided with\n",
    "        self.left = left # the left tree that is pointing to \n",
    "        self.right = right # the right tree that is pointing to\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value # the value of the tree\n",
    "        \n",
    "    # include a check, to see if this node is a leaf node or not\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \n",
    "    # initialization\n",
    "    #******************************************\n",
    "    def __init__(self, criterion='gini', max_depth=None, n_features=None, random=False):\n",
    "        \n",
    "        # record feature importance\n",
    "        self.feat_list = []\n",
    "\n",
    "        # mode selection\n",
    "        self.criterion = criterion\n",
    "        if criterion == 'gini':\n",
    "            self.measure_func = gini\n",
    "        else:\n",
    "            self.measure_func = entropy\n",
    "            \n",
    "        # stopping criteria\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        # number of features, we are not using all of the features in the tree,\n",
    "        # but just a subset of them, used for random forest\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # use random feature or not\n",
    "        self.random = random\n",
    "        \n",
    "        # keep access to he root of the node, used for traversing\n",
    "        self.root = None\n",
    "\n",
    "        \n",
    "    # fitting with helper functions\n",
    "    #****************************************\n",
    "    def fit(self, x_data, y_data, sample_weight=None):\n",
    "        if self.n_features is None:\n",
    "            self.n_features = x_data.shape[1]\n",
    "        \n",
    "        self.sample_weight = sample_weight\n",
    "        \n",
    "        # _grow_tree will return the root of the tree in the end\n",
    "        self.root = self._grow_tree(x_data, y_data)\n",
    "            \n",
    "    \n",
    "    def _grow_tree(self, x_data, y_data, depth=0,*,other_side_ydata=None):\n",
    "                                      # first set the depth to 0, while we are creating the tree\n",
    "                                      # increase it by 1\n",
    "        n_samples, n_feats = x_data.shape\n",
    "        n_lables = len(np.unique(y_data))\n",
    "        \n",
    "        # 1. check the stopping critiria: meet max depth or is a leaf node\n",
    "        # stop growing the tree and create a new node, then return it\n",
    "        # find out the most common label inside the node\n",
    "        if((self.max_depth is not None and depth >=self.max_depth) or n_lables==1 or n_lables==0):\n",
    "            if n_lables==0:\n",
    "                leaf_value = self._most_common_label(other_side_ydata)\n",
    "                return Node(value=leaf_value)\n",
    "            else:\n",
    "                leaf_value = self._most_common_label(y_data)\n",
    "                return Node(value=leaf_value)\n",
    "        \n",
    "        # pick the features we want to consider\n",
    "        if self.random==False:\n",
    "            feat_idxs = np.arange(n_feats)\n",
    "        else:\n",
    "            if self.n_features  == n_feats + 1:\n",
    "                self.n_features -= 1\n",
    "            feat_idxs = np.random.choice(np.arange(n_feats), self.n_features, replace=False)\n",
    "        \n",
    "        # 2. find the best split\n",
    "        # if can't find feature and threshold, return the node\n",
    "        best_feature, best_thresh = self._best_split(x_data, y_data, feat_idxs)\n",
    "        if best_thresh is None or best_feature is None:\n",
    "            leaf_value = self._most_common_label(y_data)\n",
    "            return Node(value=leaf_value)\n",
    "        self.feat_list.append(best_feature)\n",
    "\n",
    "        # 3. create child nodes\n",
    "        left_idxs, right_idxs = self._split(x_data[:, best_feature], best_thresh)\n",
    "        \n",
    "        # build the subtree recursively:\n",
    "        # grow left tree\n",
    "        left = self._grow_tree(x_data[left_idxs, :], y_data[left_idxs], depth+1)\n",
    "        # grow right tree\n",
    "        right = self._grow_tree(x_data[right_idxs, :], y_data[right_idxs], depth+1)\n",
    "        \n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # find the best threshold/split among all possible thresholds/split\n",
    "    def _best_split(self, x_data, y_data, feat_idxs):\n",
    "        \n",
    "        smallest_impurity = 100 # the impurity value\n",
    "        split_idx, split_threshold = None, None\n",
    "    \n",
    "        # traverse all possible features\n",
    "        for feat_idx in feat_idxs:\n",
    "            x_column = x_data[:, feat_idx]\n",
    "            # get all the possible threshold\n",
    "            thresholds = np.unique(x_column)\n",
    "            \n",
    "            # traverse all possible thresholds\n",
    "            for threshold in thresholds:\n",
    "                # calculate the information gain\n",
    "                impurity = self._information_gain(y_data, x_column, threshold)\n",
    "\n",
    "                if impurity < smallest_impurity:\n",
    "                    smallest_impurity = impurity\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "    \n",
    "    \n",
    "    def _information_gain(self, y_data, x_column, threshold):\n",
    "        \n",
    "       \n",
    "        # 1. create children\n",
    "        # split data according to current threshold\n",
    "        left_idxs, right_idxs = self._split(x_column, threshold)\n",
    "        \n",
    "        # if either side is empty, just return the the IG is 0(didn't do splitting)\n",
    "        if(len(left_idxs)==0 or len(right_idxs)==0):\n",
    "            return 100\n",
    "        \n",
    "        child_value = 0\n",
    "        # 2. calculate the weighted avg. evaluation of children\n",
    "        if self.sample_weight is None:\n",
    "            # calculate the measure_func(gini or entropy) values of left and right\n",
    "            value_l, value_r = self.measure_func(y_data[left_idxs]), self.measure_func(y_data[right_idxs])\n",
    "            \n",
    "            # calculate the weighted avg. value of children \n",
    "            n = len(y_data) # how many samples we have in y_data\n",
    "            n_l, n_r = len(left_idxs), len(right_idxs) # how many samples we have in left and right nodes\n",
    "            child_value = (n_l/n) * value_l + (n_r/n) * value_r\n",
    "        else:\n",
    "            # separate sample weights into left and right node\n",
    "            left_weight_data = self.sample_weight[left_idxs]\n",
    "            right_weight_data = self.sample_weight[right_idxs]\n",
    "            \n",
    "            # calculate the measure_func(gini or entropy) values of left and right\n",
    "            value_l, value_r = self.measure_func(y_data[left_idxs], left_weight_data), self.measure_func(y_data[right_idxs], right_weight_data)\n",
    "            \n",
    "            left_weight_sum = np.sum(left_weight_data)\n",
    "            right_weight_sum = np.sum(right_weight_data)\n",
    "            weight_sum = np.sum(self.sample_weight)\n",
    "            \n",
    "            child_value = (left_weight_sum/weight_sum) * value_l + (right_weight_sum/weight_sum) * value_r\n",
    "        \n",
    "        return child_value\n",
    "        \n",
    "    # split the x_column data into left and right nodes\n",
    "    def _split(self, x_column, threshold):\n",
    "        # indices that go to the left tree\n",
    "        left_idxs = np.argwhere(x_column < threshold).flatten()\n",
    "        # indices that go to the right tree\n",
    "        right_idxs = np.argwhere(x_column >= threshold).flatten()\n",
    "        \n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    # count the most common value inside the node\n",
    "    def _most_common_label(self, y):\n",
    "        class_label, class_count = np.unique(y, return_counts= True)\n",
    "        return class_label[np.argmax(class_count)]\n",
    "        \n",
    "    # prediction with helper function\n",
    "    #****************************************    \n",
    "    def predict(self, x_data):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in x_data])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        # x is data(row) being predicted, and node is where x is right now\n",
    "        \n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] < node.threshold:\n",
    "            # pass the left side of the tree to be traverse\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        # pass the right side of the tree to be traverse\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion=gini, max_depth=3, accuracy: 0.92\n",
      "criterion=gini, max_depth=10, accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "clf_depth3.fit(x_train, y_train)\n",
    "print(\"criterion=gini, max_depth=3, accuracy:\", accuracy_score(clf_depth3.predict(x_test), y_test))\n",
    "\n",
    "clf_depth10.fit(x_train, y_train)\n",
    "print(\"criterion=gini, max_depth=10, accuracy:\", accuracy_score(clf_depth10.predict(x_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion=gini, max_depth=3, accuracy: 0.92\n",
      "criterion=entropy, max_depth=3, accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "clf_gini.fit(x_train, y_train)\n",
    "print(\"criterion=gini, max_depth=3, accuracy:\", accuracy_score(clf_gini.predict(x_test), y_test))\n",
    "\n",
    "clf_entropy.fit(x_train, y_train)\n",
    "print(\"criterion=entropy, max_depth=3, accuracy:\", accuracy_score(clf_entropy.predict(x_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first, we count the number of appearances of each features\n",
    "feat_idxs, cnt = np.unique(clf_depth10.feat_list, return_counts=True)\n",
    "# and then sort them by number of appearances\n",
    "feat_idxs = feat_idxs[np.argsort(cnt)]\n",
    "cnt = cnt[np.argsort(cnt)]\n",
    "\n",
    "# second, we select the feature name from datafrmae to form a new list, which is in the \n",
    "# order of number of appearances\n",
    "feature = []\n",
    "for idx in feat_idxs:\n",
    "    feature.append(train_df.columns[idx])\n",
    "\n",
    "# last, we draw our graph\n",
    "plt.barh(feature, cnt, color='blue')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "                     # n_estimators = number of iterations = number of weak learners\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = [] # weights for weak classifiers\n",
    "        self.clfs = [] # list of classifiers\n",
    "    \n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        \n",
    "        n_samples, n_features = x_data.shape\n",
    "        \n",
    "        # initialize the weights\n",
    "        w = np.full(n_samples, (1/n_samples))\n",
    "        \n",
    "        # iterate through base learners\n",
    "        for i in range(self.n_estimators):\n",
    "            \n",
    "            clf_depth1 = DecisionTree(criterion='gini', max_depth=1) # initialization\n",
    "            self.clfs.append(clf_depth1)\n",
    "            self.clfs[i].fit(x_data, y_data, sample_weight=w) # fitting\n",
    "            pred = self.clfs[i].predict(x_data) # prediction\n",
    "            \n",
    "            \n",
    "            # calculate the error and update w from misclassified case\n",
    "            \n",
    "            # pick the predictions' weights where pred!=y_data\n",
    "            # x=1 if pred!=y_data\n",
    "            miss = [int(x) for x in (pred != y_data)]\n",
    "            error = np.dot(w, miss)\n",
    "\n",
    "            # misclassified: pred*y_data=-1, correct: pred*y_data=1\n",
    "            yh = [-1 if x == 1 else 1 for x in miss]\n",
    "\n",
    "            # update alpha and data weight\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            w = np.multiply(w, np.exp([-x * self.alphas[i] for x in yh]))\n",
    "\n",
    "            # normalize data weights so that the sum of data weights is 1\n",
    "            w = w / np.sum(w)\n",
    "\n",
    "    \n",
    "    def predict(self, x_data):\n",
    "        pred = np.zeros(len(x_data))\n",
    "        for i in range(len(self.clfs)):\n",
    "            weak_pred = self.clfs[i].predict(x_data)\n",
    "            weak_pred[weak_pred==0] = -1\n",
    "            pred += self.alphas[i]*weak_pred\n",
    "        \n",
    "        pred = np.sign(pred)\n",
    "        pred[pred<0] = 0\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Ada_10 = AdaBoost(n_estimators=10)\n",
    "clf_Ada_10.fit(x_train, y_train)\n",
    "y_pred = clf_Ada_10.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10\n",
      "Test-set accuarcy score:  0.95\n"
     ]
    }
   ],
   "source": [
    "print('n_estimators=10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Ada_100 = AdaBoost(n_estimators=100)\n",
    "clf_Ada_100.fit(x_train, y_train)\n",
    "y_pred = clf_Ada_100.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100\n",
      "Test-set accuarcy score:  0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "print('n_estimators=100')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators # The number of trees in the forest\n",
    "        self.max_features = int(max_features) # The number of random select features to consider \n",
    "                                              # when looking for the best split\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = [] # store each single tree\n",
    "        \n",
    "    def fit(self, x_data, y_data):\n",
    "        self.trees = []\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # boostrapping\n",
    "            if self.boostrap:\n",
    "                sample_idx = np.random.choice(len(x_data), size=len(x_data), replace=True)\n",
    "            else:\n",
    "                sample_idx = np.arrange((len(x_data)))\n",
    "            tree = DecisionTree(criterion=self.criterion, \n",
    "                                max_depth=self.max_depth, \n",
    "                                n_features=self.max_features,\n",
    "                                random=True)\n",
    "            \n",
    "            tree.fit(x_data[sample_idx], y_data[sample_idx])\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            \n",
    "    def predict(self, x_data):\n",
    "\n",
    "        y_pred = np.zeros(len(x_data), dtype='int64')\n",
    "        for i in range(len(self.trees)):\n",
    "            y_pred += self.trees[i].predict(x_data)\n",
    "\n",
    "        mask = (y_pred > (self.n_estimators / 2)) # mask is a 'True' or 'False' array\n",
    "        y_pred[mask] = 1 # set elements in y_pred to 1 if its corresponding mask is 'True'\n",
    "        y_pred[(mask == False)] = 0 # set elements in y_pred to 0 if its corresponding mask is 'False'\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10, test-set accuarcy score:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_10tree.fit(x_train, y_train)\n",
    "y_pred = clf_10tree.predict(x_test)\n",
    "print('n_estimators=10, test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100, test-set accuarcy score:  0.94\n"
     ]
    }
   ],
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_100tree.fit(x_train, y_train)\n",
    "y_pred = clf_100tree.predict(x_test)\n",
    "print('n_estimators=100, test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10, max_features=sqrt(n_features), accuracy: 0.9166666666666666\n",
      "n_estimators=10, max_features=n_features, accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n",
    "\n",
    "clf_random_features.fit(x_train, y_train)\n",
    "print(\"n_estimators=10, max_features=sqrt(n_features), accuracy:\", accuracy_score(clf_random_features.predict(x_test), y_test))\n",
    "\n",
    "clf_all_features.fit(x_train, y_train)\n",
    "print(\"n_estimators=10, max_features=n_features, accuracy:\", accuracy_score(clf_all_features.predict(x_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(x, y):\n",
    "    ## Define your model and training \n",
    "    my_model = AdaBoost(n_estimators=150)\n",
    "    my_model.fit(x, y)\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = train_your_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuarcy score:  0.9766666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = my_model.predict(x_test)\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_pred.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m500\u001b[39m, )\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d99bfbbd4799c382bebdd1fa364ad9e55f169dfd2813789b2b8ad9627c9eea4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
